---
title: "High Performance and Parallel Computing and R: Concepts and Examples"
author: 
  - Jeremiah J. Nieves
output: 
  html_document:
    toc: true
    toc_float: true
    toc_collapsed: true
    toc_depth: 2
    number_sections: true
---
<!-- Note: For formatting, this Notebook requires the bookdown package  -->

<!--  Set scrollable code and preview blocks and limit the code window size  -->
```{css, include = FALSE, echo = FALSE}
pre {
  max-height: 300px;
  overflow-y: auto;
}

pre[class] {
  max-height: 300px;
}
```

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r, setup, include = FALSE}
options(prompt = 'R> ', continue = '+ ', width = 80)
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\n \\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})
require(knitr)
require(raster)
require(randomForest)
require(ggplot2)
require(ggspatial)
require(viridis)
```

# Introduction

---

Last updated: 2021-10-01

## Purpose of the Text
This text is meant to be a example-based primer for utilising R scripts within a High Performance Computing (HPC) environment in both "serial", i.e. one at a time, and "parallel", many at a time, computing tasks. While I focus here on the Barkla HPC at the University of Liverpool, a lot of the concepts and techniques are transferable to other Linux-based HPC ecosystems with a Slurm-based task manager. These examples and guidance are based upon my experiences using the IRIDIS 4 and IRIDIS 5 HPC systems at the University of Southampton; the SUMMIT HPC system at the University of Colorado, Boulder; and the Barkla HPC system at the University of Liverpool. I further focus on processing tasks that are CPU driven and do not address GPU driven processing, which is also possible on all of these, and most, HPC systems.

This text is not meant to be a substitute for reading the user manual and guidance literature provided by the maintainers of the HPC system you are using! It is rather meant to be complementary, providing a more step by step exemplification and explanation of some of the more core types of operations and tasks that you might encounter while processing within an HPC system. The user manuals and guidance literature tend to have broader examples of some more specialised operations and scenarios as well as be more programming language agnostic; this text focuses only on using the R statistical programming language. While this broader overview is invaluable, its purpose does not allow for more in-depth examples and explanations that are contained here. Unless otherwise noted with in-text citation, the information in this text is largely based upon the excellent Barkla user manual:

Addison, C. & M. Wang. 2021. "Some basic information on using the Barkla HPC cluster". *Advanced Research Computing, University of Liverpool*. Liverpool, UK.

This text assumes that you have an intermediate knowledge of the R statistical programming language, access permissions to your relevant HPC system, and that HPC system is Linux-based and uses Slurm as its job scheduling manager. You also need to have an SSH client installed and either a direct ethernet cable into your university/HPC network, or have a VPN connection to your university/HPC network. Here, we utilise [MobaXterm](https://mobaxterm.mobatek.net/) as our GUI-based SSH client for ease of use, but technically you an utilise command line/ terminal SSH commands as well.

Additionally, this text assumes you have a working knowledge of the Bash programming language and its use within GNU/Linux. You can find a brief overview of Linux at [opensource.com](https://opensource.com/resources/linux), a list of common Linux commands can be found at [beginnerlinuxtutorial.com](http://beginnerlinuxtutorial.com/basic-linux-commands/linux-basic-commands-tutorial/), and there is an excellent set of tutorials for Linux provided by the [University of Surrey](http://www.ee.surrey.ac.uk/Teaching/Unix/). Additionally, there is a short [Youtube video](https://www.youtube.com/watch?v=IVquJh3DXUA) introducing common Linux commands in a dynamic fashion.

Before we go further, I need to acknowledge the folks who made this text possible in some form or fashion. I'd like to thank Maksym Bondarenko for selflessly teaching me how to use an HPC and parallel program during our time at the University of Southampton. I also want to thank Cliff Addison and Manhui Wang, of the University of Liverpool Advanced Research Computing Team, for our exchanges in understanding the small but sometimes significant differences between HPC systems and helping me get set up quickly at the University of Liverpool.

Finally, this text is constructed using R 3.6.3 and consulting the Barkla manual last updated in July 2021, which was the most up to date version at the time of writing in October 2021.

## Example Data Used
For any examples using data as an input, we will be using the built-in `esoph` dataset. This dataset captures the occurrence of esophageal cancer along with corresponding age groups, alcohol consumption, and tobacco consumption via smoking. Other data used for examples will be minimal and defined within the code blocks.

## Structure of the Text
The remainder of the text is structured as follows: [Section 2](#Parallel-Computing-Introduction-and-Background) introduces the basic ideas of parallel programming as well as some common parallel programming workflows or "architectures". Templates and simple examples are provided and will be built upon in later sections. [Section 3](#HPC-Systems-and-Set-Up) introduces HPC systems in general, when they may be useful, their infrastructure, how to interface with them, and specific details of the Barkla HPC. I go further and discuss how to set up you HPC environment, install R packages locally, details of job submission and job parameters, and how to make sure the HPC team gets credit in your works. [Section 4](#Examples-of-Common-Jobs-on-an-HPC-in-R) gives example R scripts, both serial and parallel computing, that might be used in an HPC for specific scenarios. [Section 5](#Creating-Basic-HPC-Job-Submission-Scripts) gives examples of simpler HPC job submission scripts, written in Bash, for corresponding R scripts from [Section 4](#Examples-of-Common-Jobs-on-an-HPC-in-R). [Section 6](#More-Advanced-HPC-Job-Submission-Scripts) builds upon this by giving examples of more advanced HPC job submission scripts for corresponding R scripts from [Section 4](#Examples-of-Common-Jobs-on-an-HPC-in-R), including how to submit multiple tasks or multiple jobs at once. Lastly, [Section 7](#Other-Possibilities-and-Further-Learning) breifly talks about some other possibilities that we do not cover that are even more advanced or rely on additional modules or more advanced computing workflows.

# Parallel Computing Introduction and Background

---

# HPC Systems and Set Up

---

## When is a High Performance Computing (HPC) Resource Useful?
HPCs, in general, are a network of *nodes* where every node consists of one or more CPUs that have multiple *cores*, higher than usual amounts of Random Access Memory (RAM), i.e. temporary memory, and or other hardware advantages (FIGURE). Cores can be thought of as a computational "worker" that can do a single task at a time. While the CPUs are typically around the speed of commercial grade CPUs that you may find in a *local machine*. A local machine or local computer consists of a single CPU, a set of RAM, a GPU, and local harddrives. A local machine is one that is physically located on your desk or in your office that you tend to interface with directly, i.e. via a keyboard plugged into the computer that is co-located with you. An HPC is typically kept in a dedicated computing room, often very well climate controlled to keep the more powerful equipment cool, and is interacted with remotely over an internet connection. This remote connection is done via a local machine where you transfer data, code, and scripts and send job submission commands to the HPC over the connection.

FIGURE OF GENERAL HPC CONFIGURATION

HPC systems can be useful in a variety of circumstances, including:

- You need or want more computational resources than your local computer can provide, e.g. more RAM, more CPU cores, or more capable GPUs
- The same program needs to be run many times, usually with different inputs
- You want to run many processing jobs at once, likely in parallel
- You want to run some processing without locking up your local machine

So, if HPCs are so powerful and useful, then why don't we always use them? Well, programming a script that can do a one time job or even carry out a parallel job, is a bit more straightforward on a local machine. You can just run the script via RStudio or another Integrated Development Environment (IDE) and have the benefit of being able to utilise the IDE facilities and interactive environment. The benefits of an HPC are unlocked by organising your script slightly differently and making a Bash job script to instruct the HPC in how to run the *executable script*, e.g. an R script. While this sounds daunting, it is less of a demand than it might initially seem.

## Overview of the Barkla HPC System
TODO CITE THE BARKLA MANUAL HERE FOR PARAPHRASING
The majority of this text will give examples that work within the Barkla HPC system, but there may be small differences in command and syntax between HPC systems that are Linux-based and utilise Slurm for job scheduling depending on environment options set as well as versions of Linux and Slurm. At the time of writing, Barkla utilises Alces Flight Direct r2017.2 which is based on CentOS Linux 7.9.2009 operating system. It also was using Slurm 20.11.08.

As stated, an HPC consists of networked nodes, and therefore computing resources. The connections between the nodes is optimised for fast communication between the nodes and for writing onto a large disk storage (essentially a large "hard drive" for the HPC) that is shared across all the nodes. All of this is resource communication and coordination is typically managed behind the scenes. So, you typically only have to write your scripts near to normal and then interact with the job scheduler, which for Barkla is Slurm. 

Barkla is comprised of 143 nodes total, including:

- 138 *compute nodes*
- 2 *large memory nodes*
- 4 Xeon Phi *accelerator nodes*
- 5 *GPU nodes*
- 2 *visualisation nodes*
- 2 *login nodes*

Each compute node consists of 40 cores (from 2 CPUs) and 384 GB of RAM (9.6 GB/core). The large memory nodes have 40 cores as well, but 1TB of RAM for larger memory intensive jobs. Each GPU node have four Nvidia NVLink P100/V100 GPUs. I'll be honest, I have no idea what the accelerator nodes do for certain, but they seem to have something to do with GPU computations, which we are not focusing on in this text. The names of the types of nodes will depend on the specific system, e.g. IRIDIS referred to their large memory nodes as their "fat nodes", but generally these are the common types of nodes present on most HPC systems.

The visualisation and the login nodes are actually not for computation and running jobs. They are the "front door" to the HPC and are where you login, upload data and scripts, and submit jobs. Logging in is done via Secure SHell (SSH) protocol through the internet either through the direct connection via Ethernet or the established VPN. The login nodes only have two cores and 32GB of RAM, whereas each visualisation node have 40 cores and 380GB of RAM. Therefore, any large data transferring or compilation is advised to happen via the visualisation nodes. Any size data transfer tends to happen faster via the visualisation nodes. Again, different systems may call certain nodes different things, e.g. some "visualisation nodes" are referred to as "data transfer nodes". On login nodes, you can edit scripts and submit jobs and a few other lighter tasks. Interactive and _lightweight_ debugging can be done on the visualisation nodes in addition to large data transfer (either from local to HPC or vice versa).

The names of the two Barkla login nodes are: `barkla4.liv.ac.uk` and `barkla5.liv.ac.uk`.

The names of the two Barkla visualisation nodes are: `barkla6.liv.ac.uk` and `barkla7.liv.ac.uk`.

When connecting via SSH to the HPC over your secured (e.g. VPN or internal network Ethernet) connection, you will need to use your University user ID and password to gain access. Note, most universities require your user ID to have been granted HPC permissions usually via another application form. So, make sure to do this prior to any login attempt. For Barkla, you can make an application at the University's [Advanced Research Computing](https://www.liverpool.ac.uk/csd/advanced-research-computing/access/) webpage. Once you are logged in, you should see something like this (FIGURE OF SPLASH PAGE). 

```{r, echo = FALSE}
knitr::include_graphics("../Figures/BARKLA_Splash_Page.png")
```

For more details, see the Barkla (or relevant HPC) user manual.

## Logging In
For a video of how to log in to Barkla using MobaXterm, which is generally the same procedure for any HPC [see this video](https://stream.liv.ac.uk/cpdedcxw). In general though, you will require the following to log into Barkla or any University-based HPC:

- User permissions to utilise the HPC services (for Barkla see [here](https://www.liverpool.ac.uk/csd/advanced-research-computing/access/)
- A secure connection to the University network (either via a VPN or a direct Ethernet connection)
- Your computing user id, e.g. 'John A. Smith' might have the username 'jasmit01'
- Your computing password

## Disk Space, Directories, and Data Uploading

Every HPC envrionemt will be a bit different, but will typically have several default storage partitions, or top-level directories that have different purposes and attributes. On Barkla, there are four top-level directories:

- The *home* directory
- The *volatile* directory
- The *scratch* directory
- The *temporary* directory

You can carry out work on any one of these directories, but their attributes, types of works, and limits differ. 

Your *home* directory (`/mnt/home/users/<user_name>/`) is what the navigation pane, on the left side of the MobaXterm window) defaults to showing (FIGUREXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX). The *home* directory has a limit of 100,000 files or 50GB of storage, whichever occurs first. However, the *home* directory has the benefit of being backed up, hence the size limits per user. Yet, the purpose of the *home* directory is not intended to be used as the primary HPC work area, whether being the origin of jobs or the output target of jobs. Rather, the *home* directory should be used to store smaller, important, and hard to recreate scripts and job submission files as opposed to data files. Other, standard backup practices should be in place for data and utilising code repositories such as Github are recommended. To determine what your *home* directory storage quota is and how much of your quota has been used, you can utilise the `quota -s`command within the Barkla terminal pane (black right-hand side of the MobaXterm window).

Your *volatile* directory (`/mnt/data1/users/<user_name>/`) is a _shared_ file system (Network File System (NFS) protocol), meaning that it is visible/accessible from all nodes (including login nodes, visualisation nodes, and compute nodes) on the HPC (not that it is shared amongst many users). It is accessible for both job submission as well as being the target of job outputs. As implied by the name, the *volatile* directory is not backed up, so job input data, job outputs, job executable scripts, and job submission scripts should have copies stored elsewhere. There is no quota for this directory, but there is an inherent physical memory limit to directory. As such, users are asked to be courteous and remove unused files as you go in order to allow all users to carry out work.

Your *scrath* directory (`/mnt/lustre/users/<user_name>/`) is also a _shared_ file system (Lustre protocol), similar to the *volatile* directory. The different file system protocol of this directory is particualrly relevant for applications that support and or utilise parallel IO (read/write) operations. Additionally, the *scrath* directory is a suitable target location for large job output files, which the Barkla user guide defines as over several hundred MB or larger.

Your *temporary* directory (`/tmp/users/<user_name>/`) is _not_ a shared file system and is, actually, specific, i.e. local, to each node. It has fast read/write speeds and is created and assigned to each node that job is assigned to. All files in the *temporary* directory are deleted automatically after 30 days from their creation.

###  Uploading data and scripts
For a video of how to upload data to Barkla using MobaXterm, which is generally the same procedure for any HPC, [see this video](TODO MAKE THE VIDEO).

## Modules and the Environment

On Barkla, and when logging in, you will be given a base environment that is standard across all users. The environment refers to what software is installed and loaded under the user by default, without loading any others. Every user's environment is independent of all other HPC users envrionment. Additionally, the environment, and thus all loaded software, will revert back to the default upon every logout/login. This is a pretty standard set up as far as HPC systems go

That said, there is a wealth of software (hundreds) already available on with the Barkla system. These software are referred to as "*modules*". Barkla has four classes of modules: applications (`apps`), compilers (`compilers`), libraries(`libs`), and Message Passing Interface (MPI) related (`mpi`). Again, given that the focus of this guid is for geospatial applications using \proglang{R}, we will eventually focus on those.

The following commands can be typed in the MobaXterm terminal to do the following:

- `module list`: shows the currently loaded modules in your environment
- `module avail`: shows the entire list of all loadable modules available in Barkla
- `module show <module_name>`: shows information about a specific module provided in the <module_name> argument
- `module load <module_name>`: loads the specific module provided in the <module_name> argument
- `moduel unload <module_name>`: unloads the specific module provided in the <module_name> argument
- `module purge`: unloads all modules that are currently loaded

It is worth noting, that when you call `module avail` you will see that there are many different versions of software available and it will be important that you load the most appropriate one for your application.

For instance, at the time of writing, there are three versions of \proglang{R} applications on Barkla, but two options for one version.

```{bash, eval = FALSE}
apps/R/3.4.0/gcc-5.5.0+lapack-3.5.0+blas-3.6.0
apps/R/3.5.0/gcc-5.5.0+lapack-3.5.0+blas-3.6.0
apps/R/3.6.3/gcc-5.5.0+lapack-3.5.0+blas-3.6.0 *default*
apps/R/3.6.3/gcc-5.5.0+lapack-3.5.0+blas-3.6.0+x11
```


You will also notice that there are other details after the final `/` in each module name. This indicates other modules, which \proglang{R} is dependent upon, that are loaded a module is loaded. For instance, calling `module load apps/R/3.6.3/gcc-5.5.0+lapack-3.5.0+blas-3.6.0` will load \proglang{R} version 3.6.3 along with the modules `gcc-5.5.0`, `lapack-3.5.0`, and `blas-3.6.0`. These modules that are dependencies are typically libraries, with the previous example showing up under `module avail` as `libs/gcc/5.5.0`, `libs/lapack/3.5.0/gcc-5.5.0`, `libs/blas/3.6.0/gcc-5.5.0`, respectively. This means that the single line `module load apps/R/3.6.3/gcc-5.5.0+lapack-3.5.0+blas-3.6.0` is equivalent to calling:


```{bash, eval = FALSE}
module load libs/lapack/3.5.0/gcc-5.5.0
module load libs/lapack/3.5.0/gcc-5.5.0
module load libs/blas/3.6.0/gcc-5.5.0
module load apps/R/3.6.3/gcc-5.5.0+lapack-3.5.0+blas-3.6.0
```

However, if you were to run the above, you would receive a message in the terminal, following the last line, that `gcc-5.5.0`, `lapack-3.5.0`, `blas-3.6.0` were already loaded. It's not an error, but indicates that calling them all individually was unnecessary.

If there are modules that are not already on Barkla, you can contact the [Advanced Research Computing (ARC) team](hpc-support@liverpool.ac.uk) to see if they would be able to be installed and made available.

### Basic necessary modules for geocomputation in R

For basic geoprocessing in R, using common geospatial packages, will require other modules to be loaded. The common geospatial packages of \pkg{sf}, \pkg{rgdal},\pkg{spdep}, and others all require the \proglang{GDAL}, \proglang{GEOS}, and \proglang{PROJ} libraries in order to operate. Because these \proglang{R} packages are _dependent_ upon these libraries, these libraries must be loaded **before** the R module is loaded. Module load order is important!

Typically, the minimum modules that must be loaded prior to \proglang{R} being loaded or used would look like the following:

```{bash, eval = FALSE}
module load libs/proj/8.0.0/gcc-5.5.0+sqlite-3.35.4
module load libs/gdal/3.2.2/gcc-5.5.0+proj-8.0.0
module load libs/geos/3.8.1/gcc-5.5.0
module load apps/R/3.6.3/gcc-5.5.0+lapack-3.5.0+blas-3.6.0
```


### Installing R packages locally

By default, the \proglang{R} module that is loaded only has the default packages that \proglang{R} comes with. If you wish to install additional packages to \proglang{R}, you will need to do that interactively by logging into a visualisation nodes on Barkla, load a \proglang{R} module, and ,*typically* run \proglang{R} interactively. However, the first time you run R prior to installing packages, you will need to set up the envrionemnt variable `R_LIBS_USER` which specifies the location of \proglang{R} packages to be installed locally.

`R_LIBS_USER` will, by default, be set to `$HOME/gridware/share/R/{version}` where version is typically `3.6.3` at the time of writing. This directory is subject to a space quota, so Barkla recommends setting it to an area in your volatile directory. This can be done by modifying your `~/.bashrc` file with the following line at the end of it:

`export R_LIBS_USER=~/volatile/R_libs/3.6.3`

You can then run \proglang{R} interactively by simply typing `R` and hitting 'Enter' in the MobaXterm terminal. You should then see the following.

```{r, echo = FALSE}
knitr::include_graphics("../Figures/Interactive_R.png")
```
Then, as stated in the Barkla manual:

>You can then install packages as you normally would in \proglang{R} on your local machine. The only difference is that you will need to specify the "mirror", or regional server where the package is downloaded from, and then when done with your \proglang{R} session, type `q()` to quit and return to the main HPC terminal. If the directory `~/volatile/R_libs/3.6.3/` doesnâ€™t exist (this happens when you attempt to install a package for the first time), R will say it cannot install into the main area because it is not writable and then will ask if you want a personal library instead (say yes) and it will prompt to create the directory `~/volatile/R_libs/3.6.3/` as a personal library (say yes). All installed packages will be put into a library in your home directory and will be sourced and available for all jobs, meaning that you _don't_ have to re-install packages every time you log in.

Lastly, when installing packages, keep an eye on the output in the MobaXterm terminal to see if the package installed correctly. If you see that it didn't, try to see why it may not have in order to figure out a solution. A lot of times, this is because of a dependency (such as \proglang{GEOS} or \proglang{GDAL} when trying to install \pkg{sf}) not being loaded within the HPC environment. Typically, you can discern which module/library/etc. is missing via a combination of the error messages and searching these messages on forums and message boards, e.g. StackOVerflow.

## Jobs, Tasks, and Job Scheduling

HPCs run **jobs**, where a job consists of a set of programmatic orders that can contain one or more tasks. **Tasks** are the smallest unit of action that takes place within a job. For instance, if we have a job submission script with a \proglang{R} executable file and that executable file contains code to carry out parallel geo-processing on a list of different shapefiles, the individual tasks would be the processing for each individual shapefile.

Jobs are given to the HPC via a **job submission script**, written in \proglang{Bash}, which:

- logs, in text files, any error, progress, and output messages 
- defines key parameters and resources to be used in the job, e.g. time, memory, nodes and cores
- loads necessary modules
- carries out any argument handling prior to calling the main script
- executes the main script containing the tasks
- handles any post script activities such as cleanup of files

The main script, called by the job submission script and hereafter referred to as the **executable file**, for our examples will be written in \proglang{R}, but of course, can be written in others such as \proglang{Python} or \proglang{C}. Jobs can request whole nodes worth of resources, partial nodes of resources (thus allowing others to 'share' the remaining cores, with other users, that are not being used), as well as multiple nodes of resources.

The resources of the HPC are shared among all of its users, but the number of users and the resources being used at any one time can be variable. As such, the jobs submitted to be carried out by the HPC must be managed and orgnaised using a *job scheduler*. In many cases, including Barkla, this is done by a program called [Slurm](https://slurm.schedmd.com/documentation.html). Slurm schedules jobs in the *job queue* and processes them in an order based upon a combination of the amount of time and computational resources required. In general, the more time and computational resources requested, the longer the queue time will be. However, the queue time is most dependent upon existing demand for resources from other users and your previously submitted (and pending) jobs. 

Slurm can be interfaced with directly utilising the MobaXterm terminal and a set of special commands. A quick cheatsheet of the Slurm commands can be found [here](https://slurm.schedmd.com/pdfs/summary.pdf) for reference.

When a job is submitted to Slurm, it is given a job number, placed in the queue, and several base checks are carried out to make sure that the requested resources are available. When the job begins running, it is run on the first, possibly the only, core. This core sort of "coordinates" with any other cores and must be provided with information on environment variables, where the input and executable files are located, and where output files should be written. The default settings are that:

- the user's environment settings are in their `.bashrc` file
- any relative directory path for files and executables are relative to the user's home directory
- Standard output and error log files are directed to a file with the name `slurm-%j.out` and `slurm-%j.error`, where `%j` is replaced with the job number.


### Job limits
There are some limits on the time and computational resources that jobs can request as well as the number of jobs you can have running concurrently and the number of jobs you can have queued. At most, you can have 1000 jobs in the queue or running; any additional jobs submitted when this limit has been reached will result in an error. A way around this can be in how your group jobs and tasks, e.g. you could take 1000 1-minute jobs and regroup/code them to run as 10 100-minute jobs. Further, users are limited to using 400 CPUS cores at a given time. more CPU cores can be requested, but no additional jobs will run until the number being concurrently used by a given user drops below that threhold. This can be overridden by submitting a job with a "high" quality of serivce (QOS) option, i.e. `#SBATCH--qos=large` or `--qos=large`, attached to the job. Further details on QOS can be foudn in the Barkla User Guide.

There are additional limits within Barkla, partially dependent upon which partition a node falls in, but we cover these in the [Partitions subsection](#Partitions).

Parallel jobs are carried out on one or more *dedicated* nodes, i.e. not shared with any other user/job, and serial jobs (or parallel *tasks*) on a given node are dependent only on the number of cores within the node. We are going to focus primarily on the parallel type jobs with only some simple serial jobs covered as building blocks to better understand the parallel type jobs.

## Partitions
In addition to different types of nodes, nodes can exist within different *partitions*, or groups of nodes, of the HPC system. These are described, along with resource limits, in the following table.

```{r, echo = FALSE, results = "asis"}
partitions_df <- data.frame("Partition Name"=c("lowpriority","cooper","krabbenhoft","langfeld","rosseinsky","thoefilis","troisi","soldini","schaich","nodes","long","himem","phi","gpu"),
                            "Number of Nodes"=c(75,16,3,8,20,10,10,1,7,61,8,2,4,3),
                            "Memory Avaialble (MB)"=c(rep("380000",11),"1100000","192000","380000+"),
                            "Time Limit (D-HH:MM:SS)"=c("1-00:00:00","10-00:00:00","15-00:00:00",rep("5-00:00:00",6),"3-00:00:00","7-00:00:00","6-00:00:00","3-00:00:00","3-00:00:00"))
kable(partitions_df, caption = "Table 1. Partitions in Barkla with characteristics")
```

Not all of these partitions are accessible by general users; some of the partitions are nodes and resources that were purchased by specific departments/projects/research groups. These restricted or dedicated partitions are: `cooper`, `krabbenhoft`, `langfeld`, `rosseinsky`, `theofilis`, `troisi`, `soldini`, and `schaich`. All users have access to the `nodes`, `long`, `himem`, `phi`, and `gpu/gpuc` partitions as well as the `lowpriority` partition. The default partition is `nodes`. The `long` partition is for jobs requiring up to (current limit) 7 days long. The `himem` partition consists of two nodes with 1.1 TB of RAM each, the `phi` node consists of four KNL Xeon Phi nodes, the `gpu` node contains three GPU nodes, and the `gpuc` partition contains two GPU nodes.

The `lowpriority` partition is a bit special. Essentially, all of the dedicated nodes are also within the `lowpriority` partition and is set up so that unused dedicated resources might be utilised by other users when idle. However, if those resources become needed by the dedicated users who purchased the nodes, then your job on the `lowpriority` partition might be cancelled mid-job to prioritise the node owners. If this happens, the job will be restarted on other available nodes, if possible, or requeued or cancelled otherwise. More details on this, how to submit a job to a specific partition, and other related options are given in the Barkla User Guide.


## Acknowledging the HPC Team!
All users of the ARC HPC facilities hosted at Liverpool should use the following text to name and acknowledge their use of the facilities in published works (including but not limited to papers, conference proceedings, presentations, posters, blog and other social media posts):

"This work was undertaken on Barkla, part of the High Performance Computing facilities at the University of Liverpool, UK."

Other HPC systems will likely have their own versions of this, but it takes a lot of hard work to keep these systems up and running, documentation current, and general daily support for researchers. So, give credit where credit is due!

# Examples of Common Jobs on an HPC in R

---



# Creating Basic HPC Job Submission Scripts

---

## Single Job


# More Advanced HPC Job Submission Scripts

---

## Array Jobs
### One job, many parallel tasks with R managing
### Many jobs, single tasks with MPI managing

# Other Possibilities and Further Learning

# References

---


This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*. 

```{r eval = FALSE}
plot(cars)
```

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.
```{r eval = FALSE}
##  https://github.com/worldpopglobal/wpUtilities/blob/master/R/wpTimeDiff.R
##  Function for calculating diferences in elapsed time and formatting it:
wpTimeDiff <- function(start, end, frm="hms") {
  dsec <- as.numeric(difftime(end, start, units = c("secs")))
  hours <- floor(dsec / 3600)
  
  if (frm == "hms" ) {
    minutes <- floor((dsec - 3600 * hours) / 60)
    seconds <- dsec - 3600*hours - 60*minutes
    
    out = paste0(
      sapply(c(hours, minutes, seconds), function(x) {
        formatC(x, width = 2, format = "d", flag = "0")
      }), collapse = ":")
    
    return(out)
  }else{
    return(hours)
  }
}


##  Function for creating a progress bar:
##  https://github.com/worldpopglobal/wpUtilities/blob/master/R/wpProgressMessage.R
wpProgressMessage <- function(x, 
                              max = 100,
                              label=NULL) {
  
  if (is.null(label)) label = ''
  if (x != max) ar = '>' else ar = ''
  
  percent <- x / max * 100
  cat(sprintf('\r[%-50s] %d%% %s',
              paste(paste(rep('=', percent / 2), collapse = ''),'',sep = ar),
              floor(percent),
              label))
  if (x == max)
    cat('\n')
}


##  TASK FARM EXAMPLE 1:   ------------------------------------------------------
##  This task farm is set up where the task farm, i.e. all cores, is passed a 
##  series of different text values that depends on what the task index is 
##  attached to the job submission on the HPC. The overall goal was to carry out a 
##  series of simulations (defined by the separately defined and sourced 
##  simulate_coarser_units_PARALLEL function) using the random seeds as the 
##  variable parameter and 
seed_list_master <- list(1244621,16542,343433,23574,23463,
                         44930,2345335,789211,1123,15550,
                         66503,78328,86937,16522,35699,
                         72550,765,11240,37511,99567,
                         2347,76866,69844,7684,3345,
                         
                         88795,86732,12559,7693,6589,
                         65318, 9937,7646,881366,2388,
                         86443,76445,19873,29875,634529,
                         76329,48194,7459,357661,88353,
                         6583,339875,856334,24623,19640122,
                         
                         853201,83535,74594,112994,322223,
                         274,85375,9189,8832,85732,
                         88233,75559,738412,28334,89553,
                         90045,23228,26535,876409,345992,
                         255,46677,749322,86355,27532,
                         
                         5567,344465,85766,264891,183332,
                         8602,34855,86323,869345,12332,
                         86304,6623,8604,28563,68921,
                         873452,2863,96782,28953,76514,
                         849664,46658,55671,89934,907902)

##  Subset the seeds for this job:
seed_list <- seed_list_master[seed_indices]

##  Task farm function:
cluster_simulate_aggregate_units <- function(seed_list, 
                                             ...) {
  ##	Start the timer:
  tStart <- Sys.time()
  
  ##	Pull the cluster:
  cl <- makeSOCKcluster(cluster_workers)
  on.exit( stopCluster(cl) )
  
  ##	Determine the number of cores we're working with:
  nodes <- length(cl)
  
  ##	Pass off required libraries and data to the cluster workers:
  clusterEvalQ(cl, {
    require(sf)
    require(spdep)
    require(dplyr)
    require(data.table)
    require(snow)
    require(log4r)
  })
  ##  Pass off the required data and functions to the nodes in the cluster
  ##   - this includes the lists used for informing predictions, and the
  ##     task functions that create the predictions/info for each subnational 
  ##     unit:
  clusterExport(cl, 
                c("target_units",
                  "output_directory", 
                  "temporary_directory",
                  "output_tag",
                  "output_name_pattern",
                  "input_polygon_path",
                  "id_field_name",
                  "pop_field_name",
                  "pop_density_field_name",
                  "probability_scale_factor",
                  "area_field_name",
                  "seed_list",
                  "seed_skip",
                  "previous_target_units",
                  "simulate_coarser_units_PARALLEL",
                  "overwrite"))
  
  ##	Start all nodes on a prediction:
  for (i in 1:nodes) {
    ##  Send the  function call to the ith node with ith task from
    ##  the prediction_list as an argument and tag it with the value i
    sendCall(cl[[i]], simulate_coarser_units_PARALLEL, i, tag = i)
  }
  
  
  ##	Create our primary cluster processing loop, recalling that we already
  ##		have clusters running:
  cat("Total tasks to process: ", length(seed_list), "\n")
  for (i in 1:length(seed_list)) {
    ##	Receive results from a node:
    predictions <- recvOneData(cl)
    
    ##	Check if there was an error:
    if (!predictions$value$success) {
      stop("ERROR: Cluster barfed...\n\n", predictions)
    }
    
    ##	Which block are we processing:
    block <- predictions$value$tag
    # cat("Received gid: ", block, "\n")
    # flush.console()
    
    ##	Check to see if we are at the end of our tasklist:
    ni <- nodes + i
    if (ni <= length(seed_list)) {
      ##	And if not, send it to the cluster node that just gave us
      ##		our last result...
      sendCall(cl[[predictions$node]], 
               simulate_coarser_units_PARALLEL,
               ni,
               tag = ni)
    }
    tEnd <- Sys.time()
    wpProgressMessage(i,
                      max = length(seed_list),
                      label = paste0("Received simulation ", ni,
                                     " Processing Time: ",
                                     wpTimeDiff(tStart, tEnd)))
  }
}
```